*******************
Compare model sizes
*******************

In :expt:`131`, I was able to achieve very good performance on the amino acid 
prediction task.  Based on this, I wanted to determine how much I could 
simplify the model and still achieve good performance.

Note that I eventually intend to use this model to categorize images produced 
by a diffusion model.  I expect this to be a more difficult task, and so I 
expect that the smallest model identified here will not necessarily be the best 
for that.  Still, I think it will be valuable to know approximately what is 
needed for this task.

Data
====
:datadir:`scripts/20250417_amino_acid_minimal_models`

Results
=======
.. note::

  In this experiment, I used epochs that are 10x smaller than usual.  The 
  prupose was to run the validation set more frequently.  The validation set is 
  the same size as usual, though, so the validation results are comparable 
  between experiments.

This is the first batch of models I trained:

.. figure:: smallest_models.svg

  All of these models have 1 block repeat.  The equivariant models have a 
  maximum frequency of 1.  I used 11Å images for these training runs.

- The equivariant models outperform the non-equivariant models, although not 
  dramatically.

  - This is consistent with this being a pretty easy task.  I actually wasn't 
    sure if equivariance would help at all in this context.

  - Note that I designed the models to have comparable latent representation 
    sizes, which means comparable compute requirements.  The equivariant 
    models, therefore, have significantly fewer parameters.

- The models with 1/4 the number of channels as the baseline models were 
  clearly worse.

  - There wasn't a big difference between the models with 1x and 1/2x channels.

- The best models used residual blocks, but even identity blocks performed 
  reasonably well.

  - The downsampling steps in these models were performed by strided 
    convolutions.  The fact that the identity blocks performed reasonably well 
    means that these convolutions alone are enough to make predictions.

  - That said, the best predictions are made with the residual blocks.

Below is the second batch of models I trained:

.. figure:: equivariant_resblock.svg

- The ideal max frequency seems to be 1.

  - I thought that higher values would give better results, at the expense of 
    greater resource requirements.  But this doesn't seem to be the case.

  - One possible reason is that only 4 max_freq=1 channels are required to 
    create 1 invariant channel, while 10 max_freq=2 channels are required for 
    the same.
  
  - I also tried a max_freq=0 model, but it had prohibitive VRAM requirements.  
    This probably had something to do with the way the tensor product 
    nonlinearity works, but I didn't take the time to look into it.

- Additional block repeats don't seem to help.

  - This perhaps isn't surprising, seeing as how I get reasonable results even 
    when I leave the blocks out entirely.

- There doesn't seem to be any advantage from doubling the number of channels 
  relative to the baseline.

Discussion
==========
The best model for predicting amino acid identities from noise-free images 
seems to have the following properties:

- equivariance, with a maximum frequency of 1.
- 1 residual block per layer.
- ≈280 channels at the end of the encoder, and 128 channels at the beginning of 
  the MLP.

For making the same predictions on images generated by a diffusion model, 
suspect that I will need a slightly bigger model.  Although the exact set of 
hyperparameters may not be too important, as almost all of the models tested 
here achieved good results in less than 1 epoch.

