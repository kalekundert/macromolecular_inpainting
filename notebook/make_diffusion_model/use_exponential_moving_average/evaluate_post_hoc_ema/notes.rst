*********************
Evaluate post-hoc EMA
*********************

[Karras2024]_ introduces the idea of post-hoc EMA.  This involves saving â‰ˆ100 
EMA checkpoints throughout the training run, then combining the weights from 
those checkpoints to approximate any value for the exponential decay 
hyperparameter.

I'm using this technique for my first attempt to use EMA, since I have no idea 
what a good decay value would be (and [Karras2024]_ show that models can be 
very sensitive to this value).

Results
=======
.. datatable:: compare_epoch_94_metrics.csv

- The EMA models perform significantly worse that the "raw" models.

  - The three models shown above are all from the same epoch.  The only 
    difference is whether or not EMA is used.

  - Technically the EMA models are "post-hoc" models, meaning that sigma-rel 
    could be anything.  However, I chose these specific sigma-rel values (0.05 
    and 0.1) such that all of the weight would get assigned to a single 
    checkpoint, for the purpose of simplifying things.  Anecdotally, the 
    results are just as bad (or worse) no matter what sigma-rel is.

  - The actual images generated by these models are consistent with the metrics 
    in this table.

  - The EMA models produce slightly better-looking results when in "training" 
    mode.  I spent some time looking into this, because I was suspicious that 
    maybe the EMA weights weren't being applied correctly, but ultimately I 
    couldn't figure out what was happening.

    It's definitely not correct to run the models in training mode.  If nothing 
    else, this will cause batch norm layers to update their statistics without 
    complementary gradient update steps.  There might be other layers that 
    don't behave right, as well.

Discussion
==========
- I still want to try switch EMA [Li2024]_

  - The basic idea behind this method is to copy the EMA weights into the model 
    being trained at the end of each epoch.  This is still regularizing---the 
    EMA weights encourage the model to find flat regions of parameter 
    space---but it also gives the model a chance to adapt to the averaged 
    parameters.

  - I was hoping that this experiment would give some insight on the best decay 
    rate to use for this experiment, but unfortunately I

- It's possible that EMA would've worked better with a slower learning rate.
