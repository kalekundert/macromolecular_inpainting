************************************
Make neighbor location discriminator
************************************

2024/09/20:

In order to validate whether a generative model is producing high-quality 
macromolecular images, I need some means of discriminating high-quality images 
from low-quality ones.  One way of doing this is to train a self-supervised 
model on macromolecular data, then evaluate how well that model performs on 
generated samples.

I've already experimented extensively with a self-supervised "neighbor 
location" model that could be used for this purpose, see :expt:`33`.  Briefly, 
the model is given two regions of macromolecular structure, and predicts the 
location of one relative to the other.  Equivariant ResNet-style models can 
achieve better than 80% accuracy on this task, depending on the amount of 
noise.

Note that a similar idea is used to evaluate the quality of generated 2FD 
images.  For that task, the discriminator is a classifier trained on labeled 
image data.  The generator is judged based on how sharply the discriminator 
classifies each sample.  My idea is a little different, because I'm not using a 
supervised classifier, but I think it could still work.

In this experiment, my goal will be to train a high-quality discriminator.  I 
expect that I'll need a pretty accurate classifier in order to get good 
results.  In :expt:`72`, I got accuracies around 80% with 33Å views and 
relatively high amounts of noise.  Here I'll need to use smaller views, because 
I'll need to be able to fit two views in the samples generated by my diffusion 
models.  I don't know if that will be a problem of not.  I'm going to continue 
to train with noise, even though I won't use noise when evaluating generated 
samples, because I believe that adding noise helps prevent the classifier from 
"cheating" by looking for common motifs that cross from one view into the 
other.

Input size
==========

2024/09/20:

In all of my previous experience with the neighbor location task, I used views 
that were about 30Å big.  That's because I was trying to pre-train models that 
could be applied to whole domains (which are about that size).  But now I need 
to be able to extract two views from the output generated by my diffusion 
models.

To decide exactly how big my diffusion model outputs should be, and therefore 
how big my neighbor location views need to be, I looked at the aminoacyl tRNA 
synthetase design scaffolds I generated a few years ago.  I found that 33Å was 
about the smallest size that was still big enough to fit the active site, all 
of the residues that I thought might need to mutate, and enough fixed 
scaffolding to appropriately condition the diffusion process.  I expect that I 
might ultimately want bigger outputs (perhaps as big as 50Å), but for the 
purpose of designing a discriminator, all that matters is the size of the 
smallest input.

Given a 33Å image, how big should the views be? 

- I'm assuming that I'll use 1Å voxels, just because that size has worked in 
  the past, and I'd be worried about losing detail if I went much larger.  

- Odd sizes are better for equivariance, because they allow for 
  perfectly-aligned convolutions.

- I want to allow some space between the views, both to avoid making the 
  problem too easy, and because that's how I want to train the model anyways.

Accounting for all of that, I'm going to start with 15Å views.

